{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optim and Enzyme with ECCO.jl\n",
    "\n",
    "Here we check that we can retrieve and evaluate functions and their adjoints from the ECCO toy problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if !isdefined(Main,:Enzyme)\n",
    "    path0=joinpath(dirname(@__FILE__),\"..\")\n",
    "    using Pkg; Pkg.activate(path0); Pkg.instantiate()\n",
    "end\n",
    "\n",
    "using ECCO, AirSeaFluxes\n",
    "import AirSeaFluxes: bulkformulae\n",
    "import ECCO: Enzyme, Optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(fc = -3.0606099804357885, adx = [0.0, 917.7850566361462, -6.121219960871577, -0.44841281435892])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toy problem to evaluate f() at (x,y) and the adjoint of f at (x,y)\n",
    "(f,f_ad,x,y)=ECCO.toy_problems.enzyme_ex4()\n",
    "\n",
    "fc=f(x,y)\n",
    "adx=f_ad(x,y)\n",
    "(fc=fc,adx=adx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(fc = 5.191703158437428e-27, gradient_check = true)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toy problem that uses optim with the analytical gradient (g!)\n",
    "(f,g!,x0,x1,result)=ECCO.toy_problems.optim_ex2()\n",
    "dx=1e-4*(x0-x1)\n",
    "(fc=f(x1),gradient_check=f(x1)<f(x1+dx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(x0 = [0.0, 0.0], x1 = [0.999999999999928, 0.9999999999998559])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toy problem that uses optim with enzyme gradient (h!)\n",
    "(h,h!,x0,x1,result) = ECCO.toy_problems.optim_ex3()\n",
    "(x0=x0,x1=x1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optim with Enzyme and AirSeaFlux\n",
    "\n",
    "Setting up a function (f_tau) that utilizes AirSeaFlux, getting its adjoint with Enzyme, and then doing a basic optimization with a fake observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4-element Vector{Float64}:\n",
       " 300.0\n",
       "   0.001\n",
       "  -0.14443527308573367\n",
       "  10.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#= Documentation from AirSeaFluxes.jl \n",
    "bulkformulae(atemp,aqh,speed,sst,hu=10,ht=2,hq=2,zref=10,atmrho=1.2)\n",
    "\n",
    "Units:\n",
    "atemp  - mean air temperature (K)  at height ht (m)\n",
    "aqh    - mean air humidity (kg/kg) at height hq (m)\n",
    "speed  - mean wind speed (m/s)     at height hu (m)\n",
    "sst    - sea surface temperature (K)\n",
    "\n",
    "Bulk formulae formulation:\n",
    "```\n",
    "wind stress = (ust,vst) = rhoA * Cd * Ws * (del.u,del.v)\n",
    "Sensib Heat flux = fsha = rhoA * Ch * Ws * del.T * CpAir\n",
    "Latent Heat flux = flha = rhoA * Ce * Ws * del.Q * Lvap\n",
    "                 = -Evap * Lvap\n",
    "```\n",
    "\n",
    "with Cd,Ch,Ce = drag coefficient, Stanton number and\n",
    "Dalton number respectively [no-units], function of\n",
    "height & stability; and\n",
    "\n",
    "```\n",
    "Ws = wind speed = sqrt(del.u^2 +del.v^2)\n",
    "del.T = Tair - Tsurf ; del.Q = Qair - Qsurf\n",
    "```\n",
    "=#\n",
    "\n",
    "function f_tau(x::Array{Float64})\n",
    "    y = bulkformulae(x[1],x[2],x[3],x[4]).tau\n",
    "end\n",
    "function f_tau_ad!(bx2, x) \n",
    "    bx = zeros(size(x))\n",
    "    Enzyme.autodiff(Enzyme.Reverse, f_tau, Enzyme.Duplicated(x, bx))\n",
    "    bx2 .= bx\n",
    "end\n",
    "x0 = [300.,0.001,1.,10.]\n",
    "bx2 = zeros(size(x0))\n",
    "f_tau_ad!(bx2,x0)\n",
    "\n",
    "result=Optim.optimize(f_tau,f_tau_ad!,x0)\n",
    "x1=Optim.minimizer(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can add an observation to the above problem to make it marginally more meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter     Function value   Gradient norm \n",
      "     0     8.252726e-03     1.663451e-03\n",
      " * time: 0.008441925048828125\n",
      "     1     5.474962e-03     1.195887e-03\n",
      " * time: 0.3094038963317871\n",
      "     2     1.856151e-06     2.010559e-05\n",
      " * time: 0.30954790115356445\n",
      "     3     2.534444e-12     2.346009e-08\n",
      " * time: 0.30970001220703125\n",
      "     4     8.324333e-28     4.251711e-16\n",
      " * time: 0.3098430633544922\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.09999999999997115"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a cost function that is the squared error between an observation and tau\n",
    "function J_tau(x::Vector{Float64})\n",
    "    # hard coded observation\n",
    "    y_obs = 0.1\n",
    "    y = bulkformulae(x[1],x[2],x[3],x[4]).tau\n",
    "    \n",
    "    # cost function\n",
    "    J = abs(y-y_obs)^2\n",
    "end\n",
    "\n",
    "# get the adjoint of the cost function\n",
    "function J_tau_ad!(bx2, x) \n",
    "    bx = zeros(size(x))\n",
    "    Enzyme.autodiff(Enzyme.Reverse, J_tau, Enzyme.Duplicated(x, bx))\n",
    "    bx2 .= bx\n",
    "end\n",
    "\n",
    "x0 = [300.,0.001,1.,10.]\n",
    "\n",
    "# evaluate the gradient at x0 (just for testing)\n",
    "bx2 = zeros(size(x0))\n",
    "J_tau_ad!(bx2,x0)\n",
    "\n",
    "# optimization with the cost function and the enzyme generated gradient \n",
    "result=Optim.optimize(J_tau, J_tau_ad!, x0, Optim.Options(show_trace=true))\n",
    "x1=Optim.minimizer(result)\n",
    "\n",
    "# check that tau at x1 is close to y_obs\n",
    "y1 = bulkformulae(x1[1],x1[2],x1[3],x1[4]).tau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The section takes the above code and makes it more configurable by making the observation an argument to the cost function. We then use a closure to get a cost function that is fixed to the given observation. We take the adjoint of the closure with Enzyme, and then do an optimization to get the inputs that minimize the error between bulkformulae.tau and the observation. The syntax here is important! Enzyme is picky about how the closure is defined, which is apparently a known issue. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09999999999997115"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cost function with the obs as an argument\n",
    "x0 = [300.,0.001,1.,10.]\n",
    "\n",
    "function J(x::Vector{Float64},y_obs)\n",
    "    y = bulkformulae(x[1],x[2],x[3],x[4]).tau\n",
    "    # cost function\n",
    "    J = (y-y_obs)^2\n",
    "    return J\n",
    "end\n",
    "\n",
    "# create a new \"closure\" that will keep y_obs constant\n",
    "y_obs = 0.1\n",
    "\n",
    "function cost_closure(y_obs)\n",
    "    return x -> J(x,y_obs)\n",
    "end\n",
    "cost = cost_closure(y_obs)\n",
    "\n",
    "# evaluate cost at x0 to see if this works\n",
    "# cost(x0) = 0.008252726417096927\n",
    "\n",
    "# get the adjoint of this new closure function that accounts for the constant obs\n",
    "function cost_ad!(bx2, x) \n",
    "    bx = zeros(size(x))\n",
    "    Enzyme.autodiff(Enzyme.Reverse, cost, Enzyme.Duplicated(x, bx))\n",
    "    bx2 .= bx\n",
    "end\n",
    "\n",
    "# for testing: evaluate the gradient at x0\n",
    "bx2 = zeros(size(x0))\n",
    "cost_ad!(bx2,x0)\n",
    "\n",
    "# optimization with the cost function and it's adjoint \n",
    "# observations are accounted for and constant\n",
    "result=Optim.optimize(cost, cost_ad!, x0)\n",
    "x1=Optim.minimizer(result)\n",
    "\n",
    "# check that tau at x1 is close to y_obs\n",
    "y1 = bulkformulae(x1[1],x1[2],x1[3],x1[4]).tau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.5",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
